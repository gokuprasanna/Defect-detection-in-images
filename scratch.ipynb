{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a932cfff98860d59",
   "metadata": {},
   "source": [
    "This is a ipython notebook for testing functions and some rough code\n",
    "\n",
    "Create a toy data generating model that generates videos of white background with noise introduced randomly\n",
    "\n",
    "Create a toy model that detects and classifies the\n",
    "\n",
    "create a work flow from toy data generation and fault detection\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aeab7d17965bc4f",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def imgen(f=100):\n",
    "    \"\"\"\n",
    "    function to generate as many image frames as necessary for a video\n",
    "    each frame \n",
    "    \"\"\"\n",
    "    batch_tensor = 255*torch.ones(*(20, 3, 1080, 1080))\n",
    "    next_frame = batch_tensor[0]\n",
    "\n",
    "    pass\n",
    "# sample input (10 RGB images containing just Gaussian Noise)\n",
    "batch_tensor = 255*torch.ones(*(20, 3, 800, 600))   # (N, C, H, W)\n",
    "\n",
    "\n",
    "# make grid (2 rows and 5 columns) to display our 10 images\n",
    "grid_img = torchvision.utils.make_grid(batch_tensor, nrow=5)\n",
    "\n",
    "# check shape\n",
    "grid_img.shape\n",
    "# torch.Size([3, 518, 1292])\n",
    "plt.imshow(grid_img.permute(1, 2, 0))\n",
    "plt.xticks([]),plt.yticks([])\n",
    "plt.gca().axes.xaxis.set_ticklabels([])\n",
    "plt.gca().axes.yaxis.set_ticklabels([])\n",
    "plt.grid(alpha = 1)\n",
    "plt.savefig(\"conveyor_texture.jpg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2647adcaef07b4d2",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import pygame\n",
    "import sys\n",
    "\n",
    "# Initialize Pygame\n",
    "pygame.init()\n",
    "\n",
    "# Screen dimensions\n",
    "SCREEN_WIDTH = 800\n",
    "SCREEN_HEIGHT = 800\n",
    "FPS = 60\n",
    "\n",
    "# Colors\n",
    "BLACK = (0, 0, 0)\n",
    "WHITE = (255, 255, 255)\n",
    "\n",
    "# Conveyor settings\n",
    "CONVEYOR_WIDTH = 1000  # Conveyor image width (longer than screen width to simulate scrolling)\n",
    "CONVEYOR_HEIGHT = 1000\n",
    "CONVEYOR_SPEED = 2  # Pixels per frame\n",
    "\n",
    "# Create the screen\n",
    "screen = pygame.display.set_mode((SCREEN_WIDTH, SCREEN_HEIGHT))\n",
    "pygame.display.set_caption(\"Clothes on Conveyor Belt Simulation\")\n",
    "\n",
    "# Load a texture to simulate clothes (replace with any image you want)\n",
    "conveyor_texture = pygame.image.load(\"conveyor_texture.jpg\")\n",
    "conveyor_texture = pygame.transform.scale(conveyor_texture, (CONVEYOR_WIDTH, CONVEYOR_HEIGHT))\n",
    "\n",
    "# Position and movement variables\n",
    "conveyor_x = 0\n",
    "\n",
    "# Camera (viewport) settings\n",
    "camera_view_height = 200\n",
    "camera_y = SCREEN_HEIGHT // 2 - camera_view_height // 2\n",
    "\n",
    "# Main loop\n",
    "clock = pygame.time.Clock()\n",
    "running = True\n",
    "\n",
    "while running:\n",
    "    # Event handling\n",
    "    for event in pygame.event.get():\n",
    "        if event.type == pygame.QUIT:\n",
    "            running = False\n",
    "\n",
    "    # Move the conveyor\n",
    "    conveyor_x -= CONVEYOR_SPEED  # Move left\n",
    "    if conveyor_x <= -CONVEYOR_WIDTH // 2:\n",
    "        conveyor_x = 0  # Loop back the conveyor\n",
    "\n",
    "    # Draw background\n",
    "    screen.fill(BLACK)\n",
    "\n",
    "    # Draw the conveyor belt (simulate endless scrolling)\n",
    "    screen.blit(conveyor_texture, (conveyor_x, SCREEN_HEIGHT // 2 - CONVEYOR_HEIGHT // 2))\n",
    "    screen.blit(conveyor_texture, (conveyor_x + CONVEYOR_WIDTH // 2, SCREEN_HEIGHT // 2 - CONVEYOR_HEIGHT // 2))\n",
    "\n",
    "    # Simulate the camera capture as a cropped rectangle\n",
    "    camera_capture = pygame.Surface((SCREEN_WIDTH, camera_view_height))\n",
    "    camera_capture.blit(screen, (0, 0), (0, camera_y, SCREEN_WIDTH, camera_view_height))\n",
    "    pygame.draw.rect(screen, WHITE, (0, camera_y, SCREEN_WIDTH, camera_view_height), 2)  # Camera frame border\n",
    "\n",
    "    # Update the display\n",
    "    pygame.display.flip()\n",
    "\n",
    "    # Maintain frame rate\n",
    "    clock.tick(FPS)\n",
    "\n",
    "# Quit Pygame\n",
    "pygame.quit()\n",
    "sys.exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3e7d5d30cd1d30",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import pygame\n",
    "import sys\n",
    "import random\n",
    "\n",
    "# Initialize Pygame\n",
    "pygame.init()\n",
    "\n",
    "# Screen dimensions\n",
    "SCREEN_WIDTH = 800\n",
    "SCREEN_HEIGHT = 600\n",
    "FPS = 60\n",
    "\n",
    "# Colors\n",
    "BLACK = (0, 0, 0)\n",
    "WHITE = (255, 255, 255)\n",
    "\n",
    "# Conveyor settings\n",
    "CONVEYOR_WIDTH = 1600  # Conveyor image width\n",
    "CONVEYOR_HEIGHT = 1600\n",
    "CONVEYOR_SPEED = 2  # Pixels per frame\n",
    "\n",
    "# Create the screen\n",
    "screen = pygame.display.set_mode((SCREEN_WIDTH, SCREEN_HEIGHT))\n",
    "pygame.display.set_caption(\"Clothes on Conveyor Belt Simulation with Pixel Errors\")\n",
    "\n",
    "# Load a texture to simulate clothes (replace with any image you want)\n",
    "conveyor_texture = pygame.image.load(\"conveyor_texture.jpg\")\n",
    "conveyor_texture = pygame.transform.scale(conveyor_texture, (CONVEYOR_WIDTH, CONVEYOR_HEIGHT))\n",
    "\n",
    "# Convert texture to a Surface that allows pixel manipulation\n",
    "conveyor_texture = conveyor_texture.convert()\n",
    "\n",
    "# Position and movement variables\n",
    "conveyor_x = 0\n",
    "\n",
    "# Camera (viewport) settings\n",
    "camera_view_height = 200\n",
    "camera_y = SCREEN_HEIGHT // 2 - camera_view_height // 2\n",
    "\n",
    "def add_random_pixel_errors(surface, error_count=100):\n",
    "    \"\"\"Adds random pixel errors to a Pygame surface.\"\"\"\n",
    "    width, height = surface.get_size()\n",
    "    for _ in range(error_count):\n",
    "        # Pick a random pixel on the texture\n",
    "        x = random.randint(0, width - 1)\n",
    "        y = random.randint(0, height - 1)\n",
    "        \n",
    "        # Assign a random color to the pixel\n",
    "        random_color = (random.randint(0, 255), random.randint(0, 255), random.randint(0, 255))\n",
    "        surface.set_at((x, y), random_color)\n",
    "\n",
    "# Main loop\n",
    "clock = pygame.time.Clock()\n",
    "running = True\n",
    "\n",
    "while running:\n",
    "    # Event handling\n",
    "    for event in pygame.event.get():\n",
    "        if event.type == pygame.QUIT:\n",
    "            running = False\n",
    "\n",
    "    # Add random pixel errors to the conveyor texture\n",
    "    add_random_pixel_errors(conveyor_texture, error_count=10)\n",
    "\n",
    "    # Move the conveyor\n",
    "    conveyor_x -= CONVEYOR_SPEED  # Move left\n",
    "    if conveyor_x <= -CONVEYOR_WIDTH // 2:\n",
    "        conveyor_x = 0  # Loop back the conveyor\n",
    "\n",
    "    # Draw background\n",
    "    screen.fill(BLACK)\n",
    "\n",
    "    # Draw the conveyor belt (simulate endless scrolling)\n",
    "    screen.blit(conveyor_texture, (conveyor_x, SCREEN_HEIGHT // 2 - CONVEYOR_HEIGHT // 2))\n",
    "    screen.blit(conveyor_texture, (conveyor_x + CONVEYOR_WIDTH // 2, SCREEN_HEIGHT // 2 - CONVEYOR_HEIGHT // 2))\n",
    "\n",
    "    # Simulate the camera capture as a cropped rectangle\n",
    "    camera_capture = pygame.Surface((SCREEN_WIDTH, camera_view_height))\n",
    "    camera_capture.blit(screen, (0, 0), (0, camera_y, SCREEN_WIDTH, camera_view_height))\n",
    "    pygame.draw.rect(screen, WHITE, (0, camera_y, SCREEN_WIDTH, camera_view_height), 2)  # Camera frame border\n",
    "\n",
    "    # Update the display\n",
    "    pygame.display.flip()\n",
    "\n",
    "    # Maintain frame rate\n",
    "    clock.tick(FPS)\n",
    "\n",
    "# Quit Pygame\n",
    "pygame.quit()\n",
    "sys.exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1469a09d0f20c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pygame\n",
    "from pygame.locals import *\n",
    "\n",
    "# Initialize the game engine\n",
    "pygame.init()\n",
    "\n",
    "# Set up some constants\n",
    "WIDTH, HEIGHT = 800, 600\n",
    "FPS = 30\n",
    "\n",
    "class ClothingStretch:\n",
    "    def __init__(self):\n",
    "        self.window = pygame.display.set_mode((WIDTH, HEIGHT))\n",
    "        self.clock = pygame.time.Clock()\n",
    "        self.objects = []\n",
    "    \n",
    "    def add_object(self, obj):\n",
    "        self.objects.append(obj)\n",
    "    \n",
    "    def run(self):\n",
    "        running = True\n",
    "        while running:\n",
    "            for event in pygame.event.get():\n",
    "                if event.type == QUIT:\n",
    "                    running = False\n",
    "            \n",
    "            # Update the position of all objects\n",
    "            for obj in self.objects:\n",
    "                obj.update()\n",
    "                \n",
    "            # Draw everything\n",
    "            self.window.fill((255, 255, 255))\n",
    "            for obj in self.objects:\n",
    "                obj.draw(self.window)\n",
    "            \n",
    "            pygame.display.flip()\n",
    "            self.clock.tick(FPS)\n",
    "        \n",
    "        pygame.quit()\n",
    "\n",
    "class ClothingObject:\n",
    "    def __init__(self, x, y, speed):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.speed = speed\n",
    "    \n",
    "    def update(self):\n",
    "        # Here you would define how your object moves (e.g., with physics)\n",
    "        pass\n",
    "    \n",
    "    def draw(self, window):\n",
    "        pygame.draw.rect(window, (0, 0, 255), pygame.Rect(self.x, self.y, 100, 100))\n",
    "# Load video\n",
    "cap = cv2.VideoCapture('conveyer_belt.mp4')\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "frame_width = int(cap.get(3))\n",
    "frame_height = int(cap.get(4))\n",
    "size = (frame_width, frame_height)\n",
    "\n",
    "# Define the codec and create VideoWriter object\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out = cv2.VideoWriter('output.avi', fourcc, fps, size)\n",
    "\n",
    "# Create a new game instance\n",
    "game = ClothingStretch()\n",
    "\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Add the frame as a clothing object to the game\n",
    "    obj = ClothingObject(0, 0, 1)\n",
    "    game.add_object(obj)\n",
    "\n",
    "    # Write the flipped frame\n",
    "    out.write(frame)\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('Frame', frame)\n",
    "\n",
    "    # Update and draw the game\n",
    "    game.run()\n",
    "    # Break the loop if 'q' key is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything done, release the VideoCapture and destroyAllWindows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fc35b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset generation complete! 100 normal and defective images saved.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Directory structure\n",
    "DATASET_DIR = \"fabric_dataset\"\n",
    "NORMAL_DIR = os.path.join(DATASET_DIR, \"normal\")\n",
    "DEFECTIVE_DIR = os.path.join(DATASET_DIR, \"defective\")\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(NORMAL_DIR, exist_ok=True)\n",
    "os.makedirs(DEFECTIVE_DIR, exist_ok=True)\n",
    "\n",
    "# Image parameters\n",
    "IMG_WIDTH, IMG_HEIGHT = 512, 512  # Image size\n",
    "NUM_SAMPLES = 100  # Number of images per category\n",
    "ERROR_COUNT = 50  # Number of pixel defects in defective images\n",
    "\n",
    "\n",
    "def generate_fabric_texture():\n",
    "    \"\"\"\n",
    "    Generates a synthetic fabric-like texture using Perlin noise simulation.\n",
    "    \"\"\"\n",
    "    texture = np.random.randint(180, 220, (IMG_HEIGHT, IMG_WIDTH), dtype=np.uint8)\n",
    "    \n",
    "    # Adding smooth texture by blurring\n",
    "    texture = cv2.GaussianBlur(texture, (9, 9), 2)\n",
    "    \n",
    "    return texture\n",
    "\n",
    "\n",
    "def add_pixel_errors(image, error_count=100):\n",
    "    \"\"\"\n",
    "    Introduces random pixel errors in the image to simulate defects.\n",
    "    \"\"\"\n",
    "    defective_img = image.copy()\n",
    "    \n",
    "    for _ in range(error_count):\n",
    "        x = random.randint(0, IMG_WIDTH - 1)\n",
    "        y = random.randint(0, IMG_HEIGHT - 1)\n",
    "        defective_img[y, x] = random.randint(0, 255)  # Set random brightness\n",
    "    \n",
    "    return defective_img\n",
    "\n",
    "\n",
    "def save_images():\n",
    "    \"\"\"\n",
    "    Generates and saves normal and defective fabric images.\n",
    "    \"\"\"\n",
    "    for i in range(NUM_SAMPLES):\n",
    "        fabric_texture = generate_fabric_texture()\n",
    "        \n",
    "        # Save normal image\n",
    "        normal_path = os.path.join(NORMAL_DIR, f\"fabric_{i}.png\")\n",
    "        cv2.imwrite(normal_path, fabric_texture)\n",
    "        \n",
    "        # Generate defective image\n",
    "        defective_texture = add_pixel_errors(fabric_texture, ERROR_COUNT)\n",
    "        defective_path = os.path.join(DEFECTIVE_DIR, f\"fabric_defect_{i}.png\")\n",
    "        cv2.imwrite(defective_path, defective_texture)\n",
    "    \n",
    "    print(f\"Dataset generation complete! {NUM_SAMPLES} normal and defective images saved.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    save_images()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f4e1a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30932/1891517566.py:59: RuntimeWarning: overflow encountered in scalar subtract\n",
      "  defective_img[y, x] = max(0, defective_img[y, x] - DEFECT_INTENSITY)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset generation complete! 100 normal and defective images saved.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Directory structure\n",
    "DATASET_DIR = \"fabric_dataset\"\n",
    "NORMAL_DIR = os.path.join(DATASET_DIR, \"normal\")\n",
    "DEFECTIVE_DIR = os.path.join(DATASET_DIR, \"defective\")\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(NORMAL_DIR, exist_ok=True)\n",
    "os.makedirs(DEFECTIVE_DIR, exist_ok=True)\n",
    "\n",
    "# Image parameters\n",
    "IMG_WIDTH, IMG_HEIGHT = 256, 256  # Image size\n",
    "NUM_SAMPLES = 100  # Number of images per category\n",
    "NUM_DEFECTS = 5  # Number of defect clusters per image\n",
    "DEFECT_RADIUS = 5  # Max radius of each defect cluster\n",
    "DEFECT_INTENSITY = 80  # Darker pixel intensity for defects\n",
    "\n",
    "\n",
    "def generate_fabric_texture():\n",
    "    \"\"\"\n",
    "    Generates a synthetic fabric-like texture using Perlin noise simulation.\n",
    "    \"\"\"\n",
    "    texture = np.random.randint(180, 220, (IMG_HEIGHT, IMG_WIDTH), dtype=np.uint8)\n",
    "    \n",
    "    # Adding smooth texture by blurring\n",
    "    texture = cv2.GaussianBlur(texture, (9, 9), 2)\n",
    "    \n",
    "    return texture\n",
    "\n",
    "\n",
    "def add_clustered_defects(image, num_defects=5, radius=10):\n",
    "    \"\"\"\n",
    "    Adds clustered defects in the image, simulating real-world fabric defects.\n",
    "    \n",
    "    Parameters:\n",
    "        - image: Input fabric texture.\n",
    "        - num_defects: Number of defect clusters.\n",
    "        - radius: Maximum spread of each defect cluster.\n",
    "    \"\"\"\n",
    "    defective_img = image.copy()\n",
    "\n",
    "    for _ in range(num_defects):\n",
    "        # Choose a random central point for the defect\n",
    "        x_center = random.randint(radius, IMG_WIDTH - radius)\n",
    "        y_center = random.randint(radius, IMG_HEIGHT - radius)\n",
    "\n",
    "        # Create a defect cluster around the central point\n",
    "        for _ in range(random.randint(5, 15)):  # Random number of points in the cluster\n",
    "            x_offset = random.randint(-radius, radius)\n",
    "            y_offset = random.randint(-radius, radius)\n",
    "            x = np.clip(x_center + x_offset, 0, IMG_WIDTH - 1)\n",
    "            y = np.clip(y_center + y_offset, 0, IMG_HEIGHT - 1)\n",
    "\n",
    "            # Reduce brightness to simulate a fabric defect (e.g., stain, burn)\n",
    "            defective_img[y, x] = max(0, defective_img[y, x] - DEFECT_INTENSITY)\n",
    "\n",
    "    return defective_img\n",
    "\n",
    "\n",
    "def save_images():\n",
    "    \"\"\"\n",
    "    Generates and saves normal and defective fabric images.\n",
    "    \"\"\"\n",
    "    for i in range(NUM_SAMPLES):\n",
    "        fabric_texture = generate_fabric_texture()\n",
    "        \n",
    "        # Save normal image\n",
    "        normal_path = os.path.join(NORMAL_DIR, f\"fabric_{i}.png\")\n",
    "        cv2.imwrite(normal_path, fabric_texture)\n",
    "        \n",
    "        # Generate defective image with clustered defects\n",
    "        defective_texture = add_clustered_defects(fabric_texture, NUM_DEFECTS, DEFECT_RADIUS)\n",
    "        defective_path = os.path.join(DEFECTIVE_DIR, f\"fabric_defect_{i}.png\")\n",
    "        cv2.imwrite(defective_path, defective_texture)\n",
    "    \n",
    "    print(f\"Dataset generation complete! {NUM_SAMPLES} normal and defective images saved.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    save_images()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23797044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset generation complete! 100 normal and defective images saved.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Directory structure\n",
    "DATASET_DIR = \"fabric_dataset\"\n",
    "NORMAL_DIR = os.path.join(DATASET_DIR, \"normal\")\n",
    "DEFECTIVE_DIR = os.path.join(DATASET_DIR, \"defective\")\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(NORMAL_DIR, exist_ok=True)\n",
    "os.makedirs(DEFECTIVE_DIR, exist_ok=True)\n",
    "\n",
    "# Image parameters\n",
    "IMG_WIDTH, IMG_HEIGHT = 256, 256  # Image size\n",
    "NUM_SAMPLES = 100  # Number of images per category\n",
    "NUM_DEFECTS = 5  # Number of defect clusters per image\n",
    "DEFECT_RADIUS = 10  # Max radius of each defect cluster\n",
    "DEFECT_INTENSITY = 80  # Darker pixel intensity for defects\n",
    "FABRIC_COLORS = [(200, 200, 200), (150, 100, 50), (50, 150, 200)]  # Gray, Brown, Blue\n",
    "\n",
    "\n",
    "def generate_fabric_texture(color):\n",
    "    \"\"\"\n",
    "    Generates a synthetic fabric-like texture with a given color.\n",
    "    \"\"\"\n",
    "    texture = np.full((IMG_HEIGHT, IMG_WIDTH, 3), color, dtype=np.uint8)\n",
    "    noise = np.random.randint(-10, 10, (IMG_HEIGHT, IMG_WIDTH, 3), dtype=np.int8)\n",
    "    texture = np.clip(texture + noise, 0, 255).astype(np.uint8)\n",
    "    texture = cv2.GaussianBlur(texture, (7, 7), 2)\n",
    "    return texture\n",
    "\n",
    "\n",
    "def add_defects(image, num_defects=5, radius=10, defect_type=\"stain\"):\n",
    "    \"\"\"\n",
    "    Adds different types of defects (stains, scratches, patterns) to the fabric texture.\n",
    "    \"\"\"\n",
    "    defective_img = image.copy()\n",
    "    \n",
    "    for _ in range(num_defects):\n",
    "        x_center = random.randint(radius, IMG_WIDTH - radius)\n",
    "        y_center = random.randint(radius, IMG_HEIGHT - radius)\n",
    "        \n",
    "        if defect_type == \"stain\":\n",
    "            for _ in range(random.randint(5, 15)):\n",
    "                x_offset, y_offset = random.randint(-radius, radius), random.randint(-radius, radius)\n",
    "                x, y = np.clip(x_center + x_offset, 0, IMG_WIDTH - 1), np.clip(y_center + y_offset, 0, IMG_HEIGHT - 1)\n",
    "                defective_img[y, x] = (0, 0, 0)\n",
    "        \n",
    "        elif defect_type == \"scratch\":\n",
    "            thickness = random.randint(1, 3)\n",
    "            length = random.randint(10, 30)\n",
    "            angle = random.randint(0, 360)\n",
    "            x_end = int(x_center + length * np.cos(np.radians(angle)))\n",
    "            y_end = int(y_center + length * np.sin(np.radians(angle)))\n",
    "            cv2.line(defective_img, (x_center, y_center), (x_end, y_end), (0, 0, 0), thickness)\n",
    "        \n",
    "        elif defect_type == \"pattern\":\n",
    "            cv2.circle(defective_img, (x_center, y_center), radius, (0, 0, 0), -1)\n",
    "    \n",
    "    return defective_img\n",
    "\n",
    "\n",
    "def save_images():\n",
    "    \"\"\"\n",
    "    Generates and saves normal and defective fabric images.\n",
    "    \"\"\"\n",
    "    for i in range(NUM_SAMPLES):\n",
    "        fabric_color = random.choice(FABRIC_COLORS)\n",
    "        fabric_texture = generate_fabric_texture(fabric_color)\n",
    "        \n",
    "        normal_path = os.path.join(NORMAL_DIR, f\"fabric_{i}.png\")\n",
    "        cv2.imwrite(normal_path, fabric_texture)\n",
    "        \n",
    "        defect_type = random.choice([\"stain\", \"scratch\", \"pattern\"])\n",
    "        defective_texture = add_defects(fabric_texture, NUM_DEFECTS, DEFECT_RADIUS, defect_type)\n",
    "        defective_path = os.path.join(DEFECTIVE_DIR, f\"fabric_defect_{i}.png\")\n",
    "        cv2.imwrite(defective_path, defective_texture)\n",
    "    \n",
    "    print(f\"Dataset generation complete! {NUM_SAMPLES} normal and defective images saved.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    save_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "792cdab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved as fabric_conveyor_simulation.mp4\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Directories for dataset\n",
    "DATASET_DIR = \"fabric_dataset\"\n",
    "NORMAL_DIR = os.path.join(DATASET_DIR, \"normal\")\n",
    "DEFECTIVE_DIR = os.path.join(DATASET_DIR, \"defective\")\n",
    "\n",
    "# Video settings\n",
    "FRAME_WIDTH, FRAME_HEIGHT = 256, 256\n",
    "FPS = 30\n",
    "VIDEO_DURATION = 10  # seconds\n",
    "FRAME_COUNT = FPS * VIDEO_DURATION\n",
    "OUTPUT_VIDEO_PATH = \"fabric_conveyor_simulation.mp4\"\n",
    "\n",
    "# Get images from dataset\n",
    "def load_images(folder):\n",
    "    return [os.path.join(folder, img) for img in os.listdir(folder) if img.endswith(\".png\")]\n",
    "\n",
    "normal_images = load_images(NORMAL_DIR)\n",
    "defective_images = load_images(DEFECTIVE_DIR)\n",
    "all_images = normal_images + defective_images\n",
    "\n",
    "if not all_images:\n",
    "    raise ValueError(\"No images found in the dataset directories.\")\n",
    "\n",
    "# Initialize video writer\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(OUTPUT_VIDEO_PATH, fourcc, FPS, (FRAME_WIDTH, FRAME_HEIGHT))\n",
    "\n",
    "# Generate video frames\n",
    "for i in range(FRAME_COUNT):\n",
    "    img_path = np.random.choice(all_images)  # Randomly select an image\n",
    "    frame = cv2.imread(img_path)\n",
    "    frame = cv2.resize(frame, (FRAME_WIDTH, FRAME_HEIGHT))\n",
    "    out.write(frame)\n",
    "\n",
    "# Release resources\n",
    "out.release()\n",
    "print(f\"Video saved as {OUTPUT_VIDEO_PATH}\")\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "# Define dataset directories\n",
    "DATASET_DIR = \"fabric_dataset\"\n",
    "NORMAL_DIR = os.path.join(DATASET_DIR, \"normal\")\n",
    "DEFECTIVE_DIR = os.path.join(DATASET_DIR, \"defective\")\n",
    "TRAIN_DIR = os.path.join(DATASET_DIR, \"train\")\n",
    "TEST_DIR = os.path.join(DATASET_DIR, \"test\")\n",
    "\n",
    "# Train-test split ratio\n",
    "TRAIN_RATIO = 0.8\n",
    "\n",
    "# Create train and test directories\n",
    "for category in [\"normal\", \"defective\"]:\n",
    "    os.makedirs(os.path.join(TRAIN_DIR, category), exist_ok=True)\n",
    "    os.makedirs(os.path.join(TEST_DIR, category), exist_ok=True)\n",
    "\n",
    "# Function to split dataset\n",
    "def split_data(source_dir, train_dest, test_dest, train_ratio=0.8):\n",
    "    files = [f for f in os.listdir(source_dir) if f.endswith(\".png\") or f.endswith(\".jpg\")]\n",
    "    random.shuffle(files)\n",
    "    split_index = int(len(files) * train_ratio)\n",
    "    train_files, test_files = files[:split_index], files[split_index:]\n",
    "    \n",
    "    for file in train_files:\n",
    "        shutil.copy(os.path.join(source_dir, file), os.path.join(train_dest, file))\n",
    "    \n",
    "    for file in test_files:\n",
    "        shutil.copy(os.path.join(source_dir, file), os.path.join(test_dest, file))\n",
    "    \n",
    "    print(f\"Processed {len(train_files)} train and {len(test_files)} test images for {source_dir}.\")\n",
    "\n",
    "# Split normal and defective datasets\n",
    "split_data(NORMAL_DIR, os.path.join(TRAIN_DIR, \"normal\"), os.path.join(TEST_DIR, \"normal\"), TRAIN_RATIO)\n",
    "split_data(DEFECTIVE_DIR, os.path.join(TRAIN_DIR, \"defective\"), os.path.join(TEST_DIR, \"defective\"), TRAIN_RATIO)\n",
    "\n",
    "print(\"Train-test split completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53813b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FabricDefectCNN(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=262144, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=2, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "Device:cuda:0, Epoch 1/10, Loss: 2.7838\n",
      "Device:cuda:0, Epoch 2/10, Loss: 0.7786\n",
      "Device:cuda:0, Epoch 3/10, Loss: 0.6512\n",
      "Device:cuda:0, Epoch 4/10, Loss: 0.6248\n",
      "Device:cuda:0, Epoch 5/10, Loss: 0.5949\n",
      "Device:cuda:0, Epoch 6/10, Loss: 0.5182\n",
      "Device:cuda:0, Epoch 7/10, Loss: 0.4472\n",
      "Device:cuda:0, Epoch 8/10, Loss: 0.4767\n",
      "Device:cuda:0, Epoch 9/10, Loss: 0.3855\n",
      "Device:cuda:0, Epoch 10/10, Loss: 0.5681\n",
      "Model training complete and saved!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from model import *\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 1000\n",
    "IMG_SIZE = 128  # Resizing images for CNN input\n",
    "\n",
    "# Dataset paths\n",
    "DATASET_DIR = \"fabric_dataset\"\n",
    "TRAIN_DIR = f\"{DATASET_DIR}/train\"\n",
    "TEST_DIR = f\"{DATASET_DIR}/test\"\n",
    "\n",
    "# Transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = datasets.ImageFolder(root=TRAIN_DIR, transform=transform)\n",
    "test_dataset = datasets.ImageFolder(root=TEST_DIR, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "\n",
    "# Initialize model, loss, and optimizer|\n",
    "model = FabricDefectCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "print(model)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Device:{device}, Epoch {epoch+1}/{EPOCHS}, Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# Save model\n",
    "torch.save(model.state_dict(), \"fabric_defect_cnn.pth\")\n",
    "print(\"Model training complete and saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "616c6679",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_76055/2401925422.py:33: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(MODEL_PATH))\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001B[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001B[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001B[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "# Load trained model\n",
    "MODEL_PATH = \"fabric_defect_cnn.pth\"\n",
    "IMG_SIZE = 128\n",
    "\n",
    "# class FabricDefectCNN(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(FabricDefectCNN, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "#         self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "#         self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "#         self.fc1 = nn.Linear(64 * (IMG_SIZE // 2) * (IMG_SIZE // 2), 128)\n",
    "#         self.fc2 = nn.Linear(128, 2)  # Two classes: Normal, Defective\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.pool(self.relu(self.conv1(x)))\n",
    "#         x = self.relu(self.conv2(x))\n",
    "#         x = x.view(x.size(0), -1)  # Flatten\n",
    "#         x = self.relu(self.fc1(x))\n",
    "#         x = self.dropout(x)\n",
    "#         x = self.fc2(x)\n",
    "#         return x\n",
    "\n",
    "# Load model\n",
    "model = FabricDefectCNN()\n",
    "model.load_state_dict(torch.load(MODEL_PATH))\n",
    "model.eval()\n",
    "\n",
    "# Define preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Open video stream\n",
    "VIDEO_PATH = \"fabric_conveyor_simulation.mp4\"\n",
    "cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Convert frame to model input format\n",
    "    image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    image = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "    \n",
    "    # Make prediction\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        label = \"Defective\" if predicted.item() == 1 else \"Normal\"\n",
    "    \n",
    "    # Display result\n",
    "    cv2.putText(frame, f\"Status: {label}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255) if label == \"Defective\" else (0, 255, 0), 2)\n",
    "    cv2.imshow(\"Fabric Defect Detection\", frame)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21285a0f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T22:39:00.711700Z",
     "start_time": "2025-01-30T22:38:58.383999Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'FabricDefectCNN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 16\u001B[0m\n\u001B[1;32m     13\u001B[0m BATCH_SIZE \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m32\u001B[39m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;66;03m# Load model\u001B[39;00m\n\u001B[0;32m---> 16\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mFabricDefectCNN\u001B[49m()\n\u001B[1;32m     17\u001B[0m model\u001B[38;5;241m.\u001B[39mload_state_dict(torch\u001B[38;5;241m.\u001B[39mload(MODEL_PATH))\n\u001B[1;32m     18\u001B[0m model\u001B[38;5;241m.\u001B[39meval()\n",
      "\u001B[0;31mNameError\u001B[0m: name 'FabricDefectCNN' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load trained model\n",
    "MODEL_PATH = \"fabric_defect_cnn.pth\"\n",
    "IMG_SIZE = 128\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Load model\n",
    "model = FabricDefectCNN()\n",
    "model.load_state_dict(torch.load(MODEL_PATH))\n",
    "model.eval()\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Load test dataset\n",
    "TEST_DIR = \"fabric_dataset/test\"\n",
    "test_dataset = datasets.ImageFolder(root=TEST_DIR, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Evaluate model\n",
    "correct = 0\n",
    "total = 0\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        all_preds.extend(predicted.numpy())\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "# Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[\"Normal\", \"Defective\"], yticklabels=[\"Normal\", \"Defective\"])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Visualizing Weights\n",
    "conv1_weights = model.conv1.weight.data.cpu().numpy()\n",
    "fig, axes = plt.subplots(4, 8, figsize=(10, 5))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    if i < conv1_weights.shape[0]:\n",
    "        ax.imshow(conv1_weights[i, 0], cmap='gray')\n",
    "        ax.axis('off')\n",
    "plt.suptitle(\"Conv1 Weights Visualization\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Visualization complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c261ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
